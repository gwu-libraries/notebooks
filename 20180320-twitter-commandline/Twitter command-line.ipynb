{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Command-line-tools-for-wrangling-Twitter-data\" data-toc-modified-id=\"Command-line-tools-for-wrangling-Twitter-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Command-line tools for wrangling Twitter data</a></span><ul class=\"toc-item\"><li><span><a href=\"#wc\" data-toc-modified-id=\"wc-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>wc</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-the-tweets\" data-toc-modified-id=\"Count-the-tweets-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Count the tweets</a></span></li></ul></li><li><span><a href=\"#gzip\" data-toc-modified-id=\"gzip-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>gzip</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compress-a-stream-of-tweets\" data-toc-modified-id=\"Compress-a-stream-of-tweets-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Compress a stream of tweets</a></span></li><li><span><a href=\"#Uncompress-to-a-stream-of-tweets\" data-toc-modified-id=\"Uncompress-to-a-stream-of-tweets-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Uncompress to a stream of tweets</a></span></li></ul></li><li><span><a href=\"#awk\" data-toc-modified-id=\"awk-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>awk</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sample-tweets\" data-toc-modified-id=\"Sample-tweets-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Sample tweets</a></span></li></ul></li><li><span><a href=\"#split\" data-toc-modified-id=\"split-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>split</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-tweets-into-files-by-number-of-tweets\" data-toc-modified-id=\"Split-tweets-into-files-by-number-of-tweets-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Split tweets into files by number of tweets</a></span></li></ul></li><li><span><a href=\"#jq\" data-toc-modified-id=\"jq-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>jq</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-mentions\" data-toc-modified-id=\"Extract-mentions-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Extract mentions</a></span></li></ul></li><li><span><a href=\"#sort\" data-toc-modified-id=\"sort-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>sort</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sort-a-list\" data-toc-modified-id=\"Sort-a-list-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Sort a list</a></span></li></ul></li><li><span><a href=\"#uniq\" data-toc-modified-id=\"uniq-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>uniq</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-unique-values-for-a-list\" data-toc-modified-id=\"Get-the-unique-values-for-a-list-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Get the unique values for a list</a></span></li></ul></li><li><span><a href=\"#tee\" data-toc-modified-id=\"tee-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>tee</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-once,-write-twice\" data-toc-modified-id=\"Read-once,-write-twice-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Read once, write twice</a></span></li></ul></li><li><span><a href=\"#parallel\" data-toc-modified-id=\"parallel-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>parallel</a></span><ul class=\"toc-item\"><li><span><a href=\"#gzip-in-parallel\" data-toc-modified-id=\"gzip-in-parallel-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>gzip in parallel</a></span></li></ul></li><li><span><a href=\"#json2csv.py\" data-toc-modified-id=\"json2csv.py-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>json2csv.py</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-JSON-tweets-to-CSV\" data-toc-modified-id=\"Convert-JSON-tweets-to-CSV-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>Convert JSON tweets to CSV</a></span></li></ul></li><li><span><a href=\"#csvkit\" data-toc-modified-id=\"csvkit-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>csvkit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select-columns\" data-toc-modified-id=\"Select-columns-1.11.1\"><span class=\"toc-item-num\">1.11.1&nbsp;&nbsp;</span>Select columns</a></span></li><li><span><a href=\"#Merge-CSVs\" data-toc-modified-id=\"Merge-CSVs-1.11.2\"><span class=\"toc-item-num\">1.11.2&nbsp;&nbsp;</span>Merge CSVs</a></span></li><li><span><a href=\"#Filter-CSVs\" data-toc-modified-id=\"Filter-CSVs-1.11.3\"><span class=\"toc-item-num\">1.11.3&nbsp;&nbsp;</span>Filter CSVs</a></span></li><li><span><a href=\"#Join-reply-tweets-and-the-tweets-that-are-replied-to\" data-toc-modified-id=\"Join-reply-tweets-and-the-tweets-that-are-replied-to-1.11.4\"><span class=\"toc-item-num\">1.11.4&nbsp;&nbsp;</span>Join reply tweets and the tweets that are replied to</a></span></li></ul></li><li><span><a href=\"#twarc-utilities\" data-toc-modified-id=\"twarc-utilities-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>twarc utilities</a></span><ul class=\"toc-item\"><li><span><a href=\"#tweet_compliance.py\" data-toc-modified-id=\"tweet_compliance.py-1.12.1\"><span class=\"toc-item-num\">1.12.1&nbsp;&nbsp;</span>tweet_compliance.py</a></span></li><li><span><a href=\"#deduplicate.py\" data-toc-modified-id=\"deduplicate.py-1.12.2\"><span class=\"toc-item-num\">1.12.2&nbsp;&nbsp;</span>deduplicate.py</a></span></li><li><span><a href=\"#deletes.py\" data-toc-modified-id=\"deletes.py-1.12.3\"><span class=\"toc-item-num\">1.12.3&nbsp;&nbsp;</span>deletes.py</a></span></li><li><span><a href=\"#unshrtn.py\" data-toc-modified-id=\"unshrtn.py-1.12.4\"><span class=\"toc-item-num\">1.12.4&nbsp;&nbsp;</span>unshrtn.py</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command-line tools for wrangling Twitter data\n",
    "Sure, you could write a script but you can often get the job done from the command-line.\n",
    "\n",
    "This is an assortment of command-line tools that I use for wrangling Twitter data. Of course, most of these tools and techniques can be used for wrangle other types of data. If you have others, please let me know.\n",
    "\n",
    "To illustrate the tools, I retrieved the tweets posted by [@gelmanlibrary](https://twitter.com/gelmanlibrary) and [@gwtweets](https://twitter.com/gwtweets) using DocNow's [Twarc](https://github.com/docnow/twarc). Twarc is a command-line tool for retrieving data from Twitter's API.\n",
    "\n",
    "    twarc timeline gwtweets > gwtweets.jsonl\n",
    "    twarc timeline gelmanlibrary > gelmanlibrary.jsonl\n",
    "    \n",
    "`gwtweets.jsonl` and `gelmanlibrary.jsonl` are line-oriented JSON files, i.e., tweets are in JSON, with one tweet on each line.\n",
    "\n",
    "*Mac tip: To get Linux-like command `brew install coreutils`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wc\n",
    "### Count the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3223 gelmanlibrary.jsonl\n",
      "    3218 gwtweets.jsonl\n",
      "    6441 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l *.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*wc gotcha: When counting many tweets, wc -l will output a partial total and then reset the count.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gzip\n",
    "Because of the size of the datasets that I deal with, I usually gzip compress the tweets.\n",
    "### Compress a stream of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gelmanlibrary.jsonl.gz\tgwtweets.jsonl.gz\r\n"
     ]
    }
   ],
   "source": [
    "!cat gwtweets.jsonl | gzip > gwtweets.jsonl.gz\n",
    "!cat gelmanlibrary.jsonl | gzip > gelmanlibrary.jsonl.gz\n",
    "!ls *.jsonl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompress to a stream of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6441\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c *.jsonl.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## awk\n",
    "### Sample tweets\n",
    "In the following example, I create a 1 in 5 sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288 sample.json\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c *.jsonl.gz | awk 'NR % 5 == 0' > sample.json\n",
    "!wc -l sample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split\n",
    "### Split tweets into files by number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1000 tweets-00.jsonl\r\n",
      "    1000 tweets-01.jsonl\r\n",
      "    1000 tweets-02.jsonl\r\n",
      "    1000 tweets-03.jsonl\r\n",
      "    1000 tweets-04.jsonl\r\n",
      "    1000 tweets-05.jsonl\r\n",
      "     441 tweets-06.jsonl\r\n",
      "    6441 total\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c *.jsonl.gz | split --lines=1000 -d --additional-suffix=.jsonl - tweets-\n",
    "!wc -l tweets-*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jq\n",
    "[jq](https://stedolan.github.io/jq/) excels at transforming JSON data. Because jq is such a useful tool for Twitter data, we already have several blog posts dedicated to it:\n",
    "* [Getting Started Working with Twitter Data Using jq](http://nbviewer.jupyter.org/github/gwu-libraries/notebooks/blob/master/20160407-twitter-analysis-with-jq/Working-with-twitter-using-jq.ipynb)\n",
    "* [Recipes for processing Twitter data with jq](http://nbviewer.jupyter.org/github/gwu-libraries/notebooks/blob/master/20161122-twitter-jq-recipes/twitter_jq_recipes.ipynb)\n",
    "\n",
    "However, here is an example of one of its uses.\n",
    "\n",
    "### Extract mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GWAcadTech\r\n",
      "Azodiac83\r\n",
      "gelmanlibrary\r\n",
      "GWtweets\r\n",
      "GWPeterK\r\n",
      "GWUSpecColl\r\n",
      "GWTextileMuseum\r\n",
      "gelmanlibrary\r\n",
      "GWAcadTech\r\n",
      "gelmanlibrary\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c *.jsonl.gz | jq -r '.entities.user_mentions[].screen_name' > screen_names.txt\n",
    "!head -10 screen_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort\n",
    "### Sort a list\n",
    "`-r` reverses the sort order (see the example for uniq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10p\r\n",
      "3PM\r\n",
      "60Minutes\r\n",
      "60Minutes\r\n",
      "60Minutes\r\n",
      "60Minutes\r\n",
      "60Minutes\r\n",
      "6pm\r\n",
      "8\r\n",
      "8nKearns\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c *.jsonl.gz | jq -r '.entities.user_mentions[].screen_name' | sort > sorted_screen_names.txt\n",
    "!head -10 sorted_screen_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniq\n",
    "### Get the unique values for a list\n",
    "The list must be sorted first. Omit `-c` to remove counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    881 gelmanlibrary\r\n",
      "    241 GWPeterK\r\n",
      "    240 EcklesLibrary\r\n",
      "    133 GWtweets\r\n",
      "    110 StudentLiaison\r\n",
      "    104 GW_MBB\r\n",
      "     82 GWDivIT\r\n",
      "     81 GWToday\r\n",
      "     78 stemworksgw\r\n",
      "     75 GW_WBB\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c *.jsonl.gz | jq -r '.entities.user_mentions[].screen_name' | sort | uniq -c | sort -r > unique_screen_names.txt\n",
    "!head -10 unique_screen_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tee\n",
    "### Read once, write twice\n",
    "In this example, I write out a lists of tweet ids and user ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976161920133816322\n",
      "976147556987297794\n",
      "976144727610454016\n",
      "976141006365253632\n",
      "976139936696020993\n",
      "28101965\n",
      "28101965\n",
      "28101965\n",
      "28101965\n",
      "28101965\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gunzip -c gwtweets.jsonl.gz | tee >(jq -r '.user.id_str' > gwtweets-user_ids.txt) | jq -r '.id_str' > gwtweets-tweet_ids.txt\n",
    "head -5 gwtweets-tweet_ids.txt\n",
    "head -5 gwtweets-user_ids.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parallel\n",
    "parallel can really speed up processes that involve multiple files. It is also useful for repeating a task multiple times, substituting in values listed in a file.\n",
    "\n",
    "`-j` controls the number of parallel processes. You should choose an appropriate number for the number of free CPUs available.\n",
    "\n",
    "### gzip in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets-00.jsonl.gz\n",
      "tweets-01.jsonl.gz\n",
      "tweets-02.jsonl.gz\n",
      "tweets-03.jsonl.gz\n",
      "tweets-04.jsonl.gz\n",
      "tweets-05.jsonl.gz\n",
      "tweets-06.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -1 tweets-*.jsonl > src.lst\n",
    "cat src.lst | sed 's/.jsonl/.jsonl.gz/' > dest.lst\n",
    "parallel -a src.lst -a dest.lst -j 2 --xapply \"cat {1} | gzip > {2}\"\n",
    "ls tweets-*.jsonl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json2csv.py\n",
    "json2csv.py is a utility that is part of Twarc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'twarc'...\n",
      "remote: Counting objects: 2606, done.\u001b[K\n",
      "remote: Total 2606 (delta 0), reused 0 (delta 0), pack-reused 2606\u001b[K\n",
      "Receiving objects: 100% (2606/2606), 659.15 KiB | 3.42 MiB/s, done.\n",
      "Resolving deltas: 100% (1648/1648), done.\n",
      "Obtaining file:///Users/justinlittman/Data/notebooks/20180320-twitter-commandline/twarc\n",
      "Requirement already satisfied: pytest in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from twarc==1.3.9)\n",
      "Requirement already satisfied: python-dateutil in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from twarc==1.3.9)\n",
      "Requirement already satisfied: requests_oauthlib in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from twarc==1.3.9)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from pytest->twarc==1.3.9)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from pytest->twarc==1.3.9)\n",
      "Requirement already satisfied: setuptools in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from pytest->twarc==1.3.9)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from pytest->twarc==1.3.9)\n",
      "Requirement already satisfied: attrs>=17.2.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from pytest->twarc==1.3.9)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from requests_oauthlib->twarc==1.3.9)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from requests_oauthlib->twarc==1.3.9)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib->twarc==1.3.9)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib->twarc==1.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib->twarc==1.3.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib->twarc==1.3.9)\n",
      "Installing collected packages: twarc\n",
      "  Found existing installation: twarc 1.3.9\n",
      "    Uninstalling twarc-1.3.9:\n",
      "      Successfully uninstalled twarc-1.3.9\n",
      "  Running setup.py develop for twarc\n",
      "Successfully installed twarc\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/DocNow/twarc.git\n",
    "!pip install -e twarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON tweets to CSV\n",
    "The `-x` option produces Excel friendly CSV by removing newlines from text fields such as the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,tweet_url,created_at,parsed_created_at,user_screen_name,text,tweet_type,coordinates,hashtags,media,urls,favorite_count,in_reply_to_screen_name,in_reply_to_status_id,in_reply_to_user_id,lang,place,possibly_sensitive,retweet_count,reweet_or_quote_id,retweet_or_quote_screen_name,retweet_or_quote_user_id,source,user_id,user_created_at,user_default_profile_image,user_description,user_favourites_count,user_followers_count,user_friends_count,user_listed_count,user_location,user_name,user_statuses_count,user_time_zone,user_urls,user_verified\r",
      "\r\n",
      "976161920133816322,https://twitter.com/GWtweets/status/976161920133816322,Tue Mar 20 18:21:52 +0000 2018,2018-03-20 18:21:52+00:00,GWtweets,@AnahiHurtado_ @jaketapper 😊,reply,,,,,1,AnahiHurtado_,976161715716087809,822162726424313859,und,,,0,,,,\"<a href=\"\"http://twitter.com\"\" rel=\"\"nofollow\"\">Twitter Web Client</a>\",28101965,Wed Apr 01 13:10:09 +0000 2009,False,\"The official Twitter account for the George Washington University, a university actively engaging Washington and the world.\",2576,46113,1812,975,\"Washington, D.C.\",GW University,8689,Eastern Time (US & Canada),http://www.gwu.edu,True\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c gwtweets.jsonl.gz | python twarc/utils/json2csv.py -x - > gwtweets.csv\n",
    "!gunzip -c gelmanlibrary.jsonl.gz | python twarc/utils/json2csv.py -x - > gelmanlibrary.csv\n",
    "!head -2 gwtweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tip when loading a tweet CSV into Excel*: If you open up a tweet CSV with Excel, it will mishandle tweet and user ids. For example, 976161920133816322 will become 976161920133816000.\n",
    "\n",
    "To correctly import tweet CSV into Excel, select Data > Get External Data > Import File. When given the option of selecting the data type for fields, select text for all id fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csvkit\n",
    "[csvkit](http://csvkit.readthedocs.io) supports a wide variety of operations for filtering and transforming CSV files. Here are a few highlights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: csvkit in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages\n",
      "Requirement already satisfied: agate>=1.6.1 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from csvkit)\n",
      "Requirement already satisfied: agate-sql>=0.5.3 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from csvkit)\n",
      "Requirement already satisfied: six>=1.6.1 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from csvkit)\n",
      "Requirement already satisfied: agate-dbf>=0.2.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from csvkit)\n",
      "Requirement already satisfied: agate-excel>=0.2.2 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from csvkit)\n",
      "Requirement already satisfied: leather>=0.3.2 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: parsedatetime>=2.1 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: python-slugify>=1.2.1 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: pytimeparse>=1.1.5 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: Babel>=2.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: isodate>=0.5.4 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.8 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate-sql>=0.5.3->csvkit)\n",
      "Requirement already satisfied: dbfread>=2.0.5 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate-dbf>=0.2.0->csvkit)\n",
      "Requirement already satisfied: xlrd>=0.9.4 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate-excel>=0.2.2->csvkit)\n",
      "Requirement already satisfied: openpyxl>=2.3.0 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from agate-excel>=0.2.2->csvkit)\n",
      "Requirement already satisfied: future in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from parsedatetime>=2.1->agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: Unidecode>=0.04.16 in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from python-slugify>=1.2.1->agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: pytz>=0a in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from Babel>=2.0->agate>=1.6.1->csvkit)\n",
      "Requirement already satisfied: jdcal in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from openpyxl>=2.3.0->agate-excel>=0.2.2->csvkit)\n",
      "Requirement already satisfied: et-xmlfile in /Users/justinlittman/Data/notebooks/ENV/lib/python3.6/site-packages (from openpyxl>=2.3.0->agate-excel>=0.2.2->csvkit)\n"
     ]
    }
   ],
   "source": [
    "!pip install csvkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,created_at,text\r\n",
      "974067184249995265,Wed Mar 14 23:38:08 +0000 2018,\"RT @GWAcadTech: The Instructional Technology Lab is available for support, including appointments and walk-in hours https://t.co/veLDwGcbWo…\"\r\n",
      "973974487384363008,Wed Mar 14 17:29:48 +0000 2018,SO MANY #GWU people here to learn about GW Box today!   Attend one of our free productivity tools workshops this week at 1pm. Thu: Getting More Out Of Spreadsheets Fri: Google Forms https://t.co/hk8MfUMeit  All the cool kids are doing it. https://t.co/Gsiib3ZSZ5\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c id,created_at,text gelmanlibrary.csv > gelmanlibrary_cut.csv\n",
    "!head -3 gelmanlibrary_cut.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3224 gelmanlibrary.csv\r\n",
      "   3224 gelmanlibrary_cut.csv\r\n",
      "     82 gelmanlibrary_replies_to.csv\r\n",
      "   3219 gwtweets.csv\r\n",
      "   6442 merged.csv\r\n",
      "  16191 total\r\n"
     ]
    }
   ],
   "source": [
    "!csvstack gelmanlibrary.csv gwtweets.csv > merged.csv\n",
    "!wc -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter CSVs\n",
    "Here I'm finding all of the tweets that are replies and getting the tweets to which they are replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,in_reply_to_status_id\r\n",
      "970826158907514880,970825298471215105\r\n",
      "960266436827602948,960265847108526081\r\n",
      "938232259173322752,938231696012562432\r\n",
      "938136395352289281,938135859211186176\r\n"
     ]
    }
   ],
   "source": [
    "!csvgrep -c tweet_type -m reply gelmanlibrary.csv | csvcut -c id,in_reply_to_status_id > replies_to_in_reply_to.csv\n",
    "!head -5 replies_to_in_reply_to.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join reply tweets and the tweets that are replied to\n",
    "Here I'm joining some @gelmanlibrary replies to the tweets that they are a reply to.\n",
    "\n",
    "The @gelmanlibrary reply will be on the left; the tweet that is being replied to will be on the right (with field names appended with \"2\").\n",
    "\n",
    "Be careful not to use csvjoin on large CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,tweet_url,created_at,parsed_created_at,user_screen_name,text,tweet_type,coordinates,hashtags,media,urls,favorite_count,in_reply_to_screen_name,in_reply_to_status_id,in_reply_to_user_id,lang,place,possibly_sensitive,retweet_count,reweet_or_quote_id,retweet_or_quote_screen_name,retweet_or_quote_user_id,source,user_id,user_created_at,user_default_profile_image,user_description,user_favourites_count,user_followers_count,user_friends_count,user_listed_count,user_location,user_name,user_statuses_count,user_time_zone,user_urls,user_verified,tweet_url2,created_at2,parsed_created_at2,user_screen_name2,text2,tweet_type2,coordinates2,hashtags2,media2,urls2,favorite_count2,in_reply_to_screen_name2,in_reply_to_status_id2,in_reply_to_user_id2,lang2,place2,possibly_sensitive2,retweet_count2,reweet_or_quote_id2,retweet_or_quote_screen_name2,retweet_or_quote_user_id2,source2,user_id2,user_created_at2,user_default_profile_image2,user_description2,user_favourites_count2,user_followers_count2,user_friends_count2,user_listed_count2,user_location2,user_name2,user_statuses_count2,user_time_zone2,user_urls2,user_verified2\r\n",
      "558708573853974528,https://twitter.com/gelmanlibrary/status/558708573853974528,Fri Jan 23 19:31:18 +0000 2015,2015-01-23 19:31:18+00:00,gelmanlibrary,Loving the hashtag for this class: #3WeeksofGeek How can you not want to get in on that action? http://t.co/qVsLmjmRpl @LizSetto @kal58,reply,,3WeeksofGeek,,http://fanpilgrimage.wordpress.com,1,lizsetto,558704312709558273,848710944,en,,False,0,,,,\"<a href=\"\"http://twitter.com\"\" rel=\"\"nofollow\"\">Twitter Web Client</a>\",9710852,Fri Oct 26 14:31:42 +0000 2007,False,The heart of the George Washington University,329,4023,2623,99,\"Foggy Bottom, Washington, DC\",gelmanlibrary,5385,Eastern Time (US & Canada),http://library.gwu.edu/,False,https://twitter.com/lizsetto/status/558704312709558273,Fri Jan 23 19:14:23 +0000 2015,2015-01-23 19:14:23+00:00,lizsetto,\"Thanks for the digital sign,@gelmanlibrary! Learn more &amp; then sign up for #3WeeksOfGeek at http://t.co/hytwTvfvuh! http://t.co/JtBURjs6aZ\",original,,3WeeksOfGeek,https://twitter.com/LizSetto/status/558704312709558273/photo/1,http://fanpilgrimage.wordpress.com,1,,,,en,,False,1,,,,\"<a href=\"\"http://www.apple.com\"\" rel=\"\"nofollow\"\">iOS</a>\",848710944,Thu Sep 27 05:37:12 +0000 2012,False,\"Librarian, homemaker, friend of animals. 🌈✨\",5133,490,640,15,\"Somerville, MA\",Liz Settoducato,2565,Pacific Time (US & Canada),,False\r\n"
     ]
    }
   ],
   "source": [
    "!csvjoin -c in_reply_to_status_id,id gelmanlibrary.csv gelmanlibrary_replies_to.csv > gelmanlibrary_with_replies_to.csv\n",
    "!head -2 gelmanlibrary_with_replies_to.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twarc utilities\n",
    "In addition to json2csv.py, Twarc includes a number of other useful tweet utilities ([docs](https://github.com/docnow/twarc#utilities) and [complete list of scripts](https://github.com/DocNow/twarc/tree/master/utils)).\n",
    "\n",
    "Here are some of my favorites.\n",
    "\n",
    "### tweet_compliance.py\n",
    "Supports [tweet compliance](https://developer.twitter.com/en/docs/tweets/compliance/overview) by retrieving the most current versions of tweets or removing unavailable (deleted or protected)\n",
    "tweets.\n",
    "\n",
    "Also useful for splitting out deleted tweets.\n",
    "\n",
    "### deduplicate.py\n",
    "Not surpringly, deduplicates tweets. For a retweet, `--extract-retweets` will return the retweet and source tweet (i.e., the tweet that is retweeted). This is useful for extracting all of the tweets in a dataset.\n",
    "\n",
    "### deletes.py\n",
    "Attempts to determine why a tweet was deleted, e.g., tweet deleted, user protected, retweet deleted.\n",
    "\n",
    "### unshrtn.py\n",
    "Unshortens URLs contained in tweets and adds them to the tweet.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "930px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
